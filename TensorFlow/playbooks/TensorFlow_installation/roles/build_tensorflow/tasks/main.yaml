---

# Before we gather information about AVX*, SSE*, FMA, etc. instructions, let's get the
# version of gcc
- name: Determine gcc version
  shell: '{{ CC }} --version | xargs | cut -d " " -f 3'
  register: GCC_VERSION

- name: Determine gcc main version
  shell: echo '{{ GCC_VERSION.stdout }}' | cut -d"." -f 1
  register: GCC_MAIN_VERSION

- name: Determine additional release info
  shell: echo '{{ GCC_VERSION.stdout }}' | cut -d"." -f 2
  register: GCC_RELEASE


# For users who want this playbook to determine the AVX instructions on their machine...
- block:

  # Next step is to determine if we have any AVX* instructions that we can take advantage of
  - name: Determine if we have AVX instructions
    shell: |
      avx=$(lscpu | grep 'Flags' | grep avx)
      echo $avx
    register: HAVE_AVX

  - name: Determine if we have AVX2 instructions
    shell: |
      avx2=$(lscpu | grep 'Flags' | grep avx2)
      echo $avx2
    register: HAVE_AVX2

  # XXX:TODO --> Make check to see which version of gcc is being used. TensorFlow requires
  # gcc version 4.9 or greater if using -mavx512f, -mavx512dq, etc.
  - name: Determine if we have AVX512F instructions
    shell: |
      avx512f=$(lscpu | grep 'Flags' | grep avx512f)
      echo $avx512f
    register: HAVE_AVX512F

  - name: Determine if we have AVX512DQ instructions
    shell: |
      avx512dq=$(lscpu | grep 'Flags' | grep avx512dq)
      echo $avx512dq
    register: HAVE_AVX512DQ

  - name: Determine if we have AVX512CD instructions
    shell: |
      avx512cd=$(lscpu | grep 'Flags' | grep avx512cd)
      echo $avx512cd
   register: HAVE_AVX512CD

  # Next step is to determine if we have FMA instructions that we can take advantage of
  - name: Determine if we have FMA instructions
    shell: |
      fma=$(lscpu | grep 'Flags' | grep fma)
      echo $fma
    register: HAVE_FMA

  when: AVX_INSTRUCTIONS|length == 0

# If the user specified their AVX instructions...
- block:

  - name: Did the user specify AVX?
    shell: |
      echo 'avx'
    register: HAVE_AVX
    when: AVX_INSTRUCTIONS == 'avx' or AVX_INSTRUCTIONS == 'avx2' or AVX_INSTRUCTIONS == 'avx512'

  - name: Did the user specify AVX2?
    shell: |
      echo 'avx2'
    register: HAVE_AVX2
    when: AVX_INSTRUCTIONS == 'avx2' or AVX_INSTRUCTIONS == 'avx512'

  - name: If the user specified AVX512 instructions, determine if we have AVX512F instructions
    shell: |
      avx512f=$(lscpu | grep 'Flags' | grep avx512f)
      echo $avx512f
    register: HAVE_AVX512F
    when: AVX_INSTRUCTIONS == 'avx512'

  - name: If the user specified AVX512 instructions, determine if we have AVX512DQ instructions
    shell: |
      avx512dq=$(lscpu | grep 'Flags' | grep avx512dq)
      echo $avx512dq
    register: HAVE_AVX512DQ
    when: AVX_INSTRUCTIONS == 'avx512'

  - name: If the user specified AVX512 instructions, determine if we have AVX512CD instructions
    shell: |
      avx512cd=$(lscpu | grep 'Flags' | grep avx512cd)
      echo $avx512cd
    register: HAVE_AVX512CD
    when: AVX_INSTRUCTIONS == 'avx512'

 when: AVX_INSTRUCTIONS|length > 0

# Now determine SSE* instructions
- name: Determine if we have SSE2 instructions
  shell: |
    sse2=$(lscpu | grep 'Flags' | grep sse2)
    echo $sse2
  register: HAVE_SSE2

- name: Determine if we have SSE3 instructions
  shell: |
    sse3=$(lscpu | grep 'Flags' | grep sse3)
    echo $sse3
  register: HAVE_SSE3

- name: Determine if we have SSE4.1 instructions
  shell: |
    sse4_1=$(lscpu | grep 'Flags' | grep sse4_1)
    echo $sse4_1
  register: HAVE_SSE4_1

- name: Determine if we have SSE4.2 instructions
  shell: |
    sse4_2=$(lscpu | grep 'Flags' | grep sse4_2)
    echo $sse4_2
  register: HAVE_SSE4_2

# Now that we've gathered information on optimization flags, let's set them appropriately
- name: Set AVX copt flag
  shell: if [[ ! -z '{{ HAVE_AVX.stdout }}' ]]; then echo "--copt=-mavx"; else echo ""; fi
  register: ENABLE_AVX

- name: Set AVX2 copt flag
  shell: if [[ ! -z '{{ HAVE_AVX2.stdout }}' ]]; then echo "--copt=-mavx2"; else echo ""; fi
  register: ENABLE_AVX2

- name: Set AVX512 copt flag
  shell: if (( '{{ GCC_MAIN_VERSION.stdout }}' < '{{ TENSORFLOW_AVX512_MIN_GCC_VER }}' )); then echo ""; elif (( '{{ GCC_RELEASE.stdout }}' < '{{ TENSORFLOW_AVX512_MIN_GCC_REL }}' )) && (( '{{ GCC_MAIN_VERSION.stdout }}' == '{{ TENSORFLOW_AVX512_MIN_GCC_VER }}' )); then echo ""; elif [[ ! -z '{{ HAVE_AVX512F.stdout }}' ]]; then echo "--copt=-mavx512f"; elif [[ ! -z '{{ HAVE_AVX512DQ.stdout }}' ]]; then echo "--copt=-mavx512dq"; elif [[ ! -z '{{ HAVE_AVX512CD.stdout }}' ]]; then echo "--copt=-mavx512cd"; else echo ""; fi
  register: ENABLE_AVX512

- name: Set FMA copt flag
  shell: if [[ ! -z '{{ HAVE_FMA.stdout }}' ]]; then echo "--copt=-mfma"; else echo ""; fi
  register: ENABLE_FMA

- name: Set SSE2 copt flag
  shell: if [[ ! -z '{{ HAVE_SSE2.stdout }}' ]]; then echo "--copt=-msse2"; else echo ""; fi
  register: ENABLE_SSE2

- name: Set SSE3 copt flag
  shell: if [[ ! -z '{{ HAVE_SSE3.stdout }}' ]]; then echo "--copt=-msse3"; else echo ""; fi
  register: ENABLE_SSE3

- name: Set SSE4.1 copt flag
  shell: if [[ ! -z '{{ HAVE_SSE4_1.stdout }}' ]]; then echo "--copt=-msse4.1"; else echo ""; fi
  register: ENABLE_SSE4_1

- name: Set SSE4.2 copt flag
  shell: if [[ ! -z '{{ HAVE_SSE4_2.stdout }}' ]]; then echo "--copt=-msse4.2"; else echo ""; fi
  register: ENABLE_SSE4_2

# Now let's print out our final compilation flags
- debug:
    msg: "Build flags: {{ ENABLE_AVX.stdout }} {{ ENABLE_AVX2.stdout }} {{ ENABLE_AVX512.stdout }} {{ ENABLE_FMA.stdout }} {{ ENABLE_SSE2.stdout }} {{ ENABLE_SSE3.stdout }} {{ ENABLE_SSE4_1.stdout }} {{ ENABLE_SSE4_2.stdout }}"

- name: Determine main TensorFlow version
  shell: |
    "echo {{ TENSORFLOW_VERSION }} | cut -d '.' -f 1"
  register: TF_MAIN_VERSION

- name: Determine TensorFlow release
  shell: |
    "echo {{ TENSORFLOW_VERSION }} | cut -d '.' -f 2"
  register: TF_RELEASE

# Setup files for TF 2.1.0 and greater
- block:

  - name: Remove {{ TENSORFLOW_BUILD_DIR }}/third_party/py/BUILD because it's empty and useless right now
    file:
      state: absent
      path: '{{ TENSORFLOW_BUILD_DIR }}/third_party/py/BUILD'

  - name: Copy {{ TENSORFLOW_BUILD_DIR }}/third_party/toolchains/preconfig/ubuntu16.04/py/BUILD to {{ TENSORFLOW_BUILD_DIR }}/third_party/py/BUILD
    copy:
      src: '{{ TENSORFLOW_BUILD_DIR }}/third_party/toolchains/preconfig/ubuntu16.04/py/BUILD'
      dest: '{{ TENSORFLOW_BUILD_DIR }}/third_party/py/BUILD'

  when: TF_MAIN_VERSION == 2 and TF_RELEASE > 0

# For RHEL 8 only (and thus ubi8), as well as TensorFlow version > 2.0.0, we have to create symbolic links for python3.6
- block:

  - name: Edit the BUILD file to change the interpreter path from /usr/bin/python2 to /usr/bin/python3
    replace:
      regexp: '    interpreter_path = "/usr/bin/python2",'
      replace: '    interpreter_path = "/usr/bin/python3",'

  - name: Edit the BUILD file to change all 'python2.7' strings to 'python3.6m' strings
    replace:
      path: '{{ TENSORFLOW_BUILD_DIR }}/third_party/py/BUILD'
      regexp: 'python2.7'
      replace: 'python3.6m'

  - name: Create symbolic link for /usr/local/include/python3.6m by setting it to point to /usr/include/python3.6m
    file:
      src: '/usr/include/python3.6m'
      dest: '/usr/local/include/python3.6m'
      state: link

  - name: Create directory which we will use for a symbolic link w/ NumPy
    file:
      state: directory
      path: '/usr/local/lib/python3.6'

  - name: Create symbolic link for /usr/local/lib/python3.6/site-packages by setting it to point to '{{ NUMPY_INSTALL_DIR }}'
    file:
      src: '{{ NUMPY_INSTALL_DIR }}'
      dest: '/usr/local/lib/python3.6/site-packages'
      state: link

  - name: Create symbolic link that causes /usr/bin/python to point to /usr/bin/python3
    file:
      state: link
      src: '/usr/bin/python'
      dest: '/usr/bin/python3'

  when: TF_MAIN_VERSION == 2 and TF_RELEASE > 0 and RHEL_VERSION == 8

# Update 'WORKSPACE' for TensorFlow 1.x
- name: TensorFlow 1.x workaround --> update {{ TENSORFLOW_BUILD_DIR }}/WORKSPACE to add the 'io_bazel_rules_docker' http archive so that bazel rules can be cloned
  blockinfile:
    path: '{{ TENSORFLOW_BUILD_DIR }}/WORKSPACE'
    insertafter: 'load\(\"\@bazel\_tools\/\/tools\/build\_defs\/repo:http.bzl\",\ \"http_archive\",\ \"http_file"\)'
    block: |
      http_archive(
          name = "io_bazel_rules_docker",
          sha256 = "aed1c249d4ec8f703edddf35cbe9dfaca0b5f5ea6e4cd9e83e99f3b0d1136c3d",
          strip_prefix = "rules_docker-0.7.0",
          urls = ["https://github.com/bazelbuild/rules_docker/archive/v0.7.0.tar.gz"],
      )
  when: TF_MAIN_VERSION < 2

# The WORKSPACE file is missing info, so we need to add the missing info to it
- name: Edit WORKSPACE file to include 'local_python_config' so that the build doesn't fail (as per the TensorFlow maintainers' suggestion)
  blockinfile:
    path: '{{ TENSORFLOW_BUILD_DIR }}/WORKSPACE'
    insertafter: '^tf_repositories()'
    block: |
      load("//third_party/py:python_configure.bzl", "python_configure")
      python_configure(name = "local_config_python")
  when: TF_MAIN_VERSION == 2 and TF_RELEASE > 0

# Now configure TensorFlow for the CPU
- name: Configure TensorFlow for the CPU
  shell: |
    cd {{ TENSORFLOW_BUILD_DIR }} 
    ./configure
  environment:
    GCC_HOST_COMPILER_PATH: '{{ CC }}'
    CC_OPT_FLAGS: '-march=native'
    TF_DOWNLOAD_CLANG: '0'
    TF_ENABLE_XLA: '0'
    TF_NEED_COMPUTECPP: '0'
    TF_NEED_CUDA: '0'
    TF_NEED_MPI: '0'
    TF_NEED_OPENCL: '0'
    TF_NEED_OPENCL_SYCL: '0'
    TF_NEED_ROCM: '0'
    TF_NEED_TENSORRT: '0'
    TF_PYTHON_CONFIG_REPO: '@org_tensorflow//third_party/toolchains/cpus/py3'
    TF_SET_ANDROID_WORKSPACE: '0'
    PYTHON_BIN_PATH: '{{ PYTHON_BIN_PATH }}'
    PYTHON_LIB_PATH: '{{ PYTHONPATH }}'
    NUMPY_INCLUDE_DIR: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core/include/numpy'
    LD_LIBRARY_PATH: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core:{{ GCC_LIBS }}'
  when: DEVICE == 'cpu'

- block:
  # Command taken from: https://georgesterpu.github.io/compile_tensorflow.html
  - name: If using the GPU, find the exact CUDA version
    shell: /usr/local/cuda/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p'
    register: cuda_version

  # Command taken from: https://georgesterpu.github.io/compile_tensorflow.html
  - name: If using the GPU, find cuDNN version
    shell: sed -n 's/^#define CUDNN_MAJOR\s*\(.*\).*/\1/p' /usr/local/cuda/include/cudnn.h
    register: cudnn_version

  # Grab the NCCL major version
  - name: If using the GPU, find NCCL major version
    shell: sed -n 's/^#define NCCL_MAJOR\s*\(.*\).*/\1/p' '{{ NCCL_INSTALL_PATH }}/include/nccl.h'
    register: nccl_major_version

  # Grab NCCL minor version
  - name: If using the GPU, find NCCL minor version
    shell: sed -n 's/^#define NCCL_MINOR\s*\(.*\).*/\1/p' '{{ NCCL_INSTALL_PATH }}/include/nccl.h'
    register: nccl_minor_version

  # Grab NCCL patch
  - name: If using the GPU, find NCCL patch
    shell: sed -n 's/^#define NCCL_PATCH\s*\(.*\).*/\1/p' '{{ NCCL_INSTALL_PATH }}/include/nccl.h'
    register: nccl_patch

  # Print out versions
  - debug:
      msg: 'CUDA version: {{ cuda_version.stdout }}'
  - debug:
      msg: 'cuDNN version: {{ cudnn_version.stdout }}'
  - debug:
      msg: 'NCCL version: {{ nccl_major_version.stdout }}.{{ nccl_minor_version.stdout }}.{{ nccl_patch.stdout }}'

  when: DEVICE == 'gpu'

# Get TensorRT version
- block:

  - name: If using the GPU and using TensorRT, find the TensorRT major version
    shell: sed -n 's/^#define NV_TENSORRT_MAJOR\s*\(.*\).*/\1/p' '{{ TENSORRT_INSTALL_PATH }}/include/NvInferVersion.h' | cut -d ' ' -f 1
    register: tensorrt_version

  - debug:
      msg: 'TensorRT version: {{ tensorrt_version.stdout }}'

  when: DEVICE == 'gpu' and USE_TENSORRT == 'yes'

# Now configure TensorFlow for the GPU
- name: Configure TensorFlow for the GPU w/o TensorRT
  shell: |
    cd {{ TENSORFLOW_BUILD_DIR }} 
    ./configure
  environment:
    GCC_HOST_COMPILER_PATH: '{{ CC }}'
    CC_OPT_FLAGS: '-march=native'
    TF_CUDA_PATHS: '/usr/local/cuda,/usr/include,{{ NCCL_INSTALL_PATH }}'
    TF_DOWNLOAD_CLANG: '0'
    TF_ENABLE_XLA: '0'
    TF_NEED_COMPUTECPP: '0'
    TF_NEED_CUDA: '1'
    TF_NEED_MPI: '0'
    TF_NEED_OPENCL: '0'
    TF_NEED_OPENCL_SYCL: '0'
    TF_NEED_ROCM: '0'
    TF_NEED_TENSORRT: '0'
    TF_SET_ANDROID_WORKSPACE: '0'
    PYTHON_BIN_PATH: '{{ PYTHON_BIN_PATH }}'
    PYTHON_LIB_PATH: '{{ PYTHONPATH }}'
    NUMPY_INCLUDE_DIR: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core/include/numpy'
    LD_LIBRARY_PATH: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core:{{ GCC_LIBS }}'
  when: DEVICE == 'gpu' and USE_TENSORRT == 'no'

# Now configure TensorFlow for the GPU
- name: Configure TensorFlow for the GPU w/ TensorRT
  shell: |
    cd {{ TENSORFLOW_BUILD_DIR }} 
    ./configure
  environment:
    GCC_HOST_COMPILER_PATH: '{{ CC }}'
    CC_OPT_FLAGS: '-march=native'
    TF_CUDA_PATHS: '/usr/local/cuda,/usr/include,{{ NCCL_INSTALL_PATH }},{{ TENSORRT_INSTALL_PATH }}'
    TF_CUDA_VERSION: '{{ cuda_version.stdout }}'
    TF_CUDNN_VERSION: '{{ cudnn_version.stdout }}'
    TF_DOWNLOAD_CLANG: '0'
    TF_ENABLE_XLA: '0'
    TF_NCCL_VERSION: '{{ nccl_major_version.stdout }}'
    TF_NEED_COMPUTECPP: '0'
    TF_NEED_CUDA: '1'
    TF_NEED_MPI: '0'
    TF_NEED_OPENCL: '0'
    TF_NEED_OPENCL_SYCL: '0'
    TF_NEED_ROCM: '0'
    TF_NEED_TENSORRT: '1'
    TF_SET_ANDROID_WORKSPACE: '0'
    TF_TENSORRT_VERSION: '{{ tensorrt_version.stdout }}'
    PYTHON_BIN_PATH: '{{ PYTHON_BIN_PATH }}'
    PYTHON_LIB_PATH: '{{ PYTHONPATH }}'
    NUMPY_INCLUDE_DIR: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core/include/numpy'
    LD_LIBRARY_PATH: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core:{{ GCC_LIBS }}'
  when: DEVICE == 'gpu' and USE_TENSORRT == 'yes'

# Print out where the configure logs have been saved to
- debug:
    msg: "TensorFlow config logs saved to {{ TENSORFLOW_BUILD_DIR }}/configure.log"

# TensorFlow 2.1.0 requires a '/root/.bazelrc' file, so let's just copy it from `pwd`/.tf_config_bazelrc
- name: Create a symlink fo have /root/.bazelrc to point to {{ TENSORFLOW_BUILD_DIR }}/.tf_configure.bazelrc
  file:
    src: '{{ TENSORFLOW_BUILD_DIR }}/.tf_configure.bazelrc'
    dest: '/root/.bazelrc'
    state: link
  when: TF_MAIN_VERSION == 2 and TF_RELEASE > 0

# Now it's time to build TensorFlow for the CPU. We want to use the optimization flags that we
# found earlier, if applicable.
- name: Build TensorFlow for the CPU
  shell: |
    cd {{ TENSORFLOW_BUILD_DIR }}
    bazel build --copt=-mfpmath=both {{ ENABLE_AVX.stdout }} {{ ENABLE_AVX2.stdout }} {{ ENABLE_AVX512.stdout }} {{ ENABLE_FMA.stdout }} {{ ENABLE_SSE2.stdout }} {{ ENABLE_SSE3.stdout }} {{ ENABLE_SSE4_1.stdout }} {{ ENABLE_SSE4_2.stdout }} --config=opt //tensorflow/tools/pip_package:build_pip_package
  when: DEVICE == 'cpu'
  environment:
    LD_LIBRARY_PATH: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core:{{ GCC_LIBS }}'

- name: Build TensorFlow for the GPU
  shell: |
    cd {{ TENSORFLOW_BUILD_DIR }}
    bazel build --copt="-I/usr/include" --copt=-mfpmath=both {{ ENABLE_AVX.stdout }} {{ ENABLE_AVX2.stdout }} {{ ENABLE_AVX512.stdout }} {{ ENABLE_FMA.stdout }} {{ ENABLE_SSE2.stdout }} {{ ENABLE_SSE3.stdout }} {{ ENABLE_SSE4_1.stdout }} {{ ENABLE_SSE4_2.stdout }} --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
  when: DEVICE == 'gpu'
  environment:
    LD_LIBRARY_PATH: '{{ NUMPY_INSTALL_DIR }}/lib/python3.6/site-packages/numpy-{{ NUMPY_VERSION }}-py3.6-linux-x86_64.egg/numpy/core:{{ GCC_LIBS }}'
    TMP: '/tmp'
    TF_CUDA_PATHS: '/usr/local/cuda,/usr/include,{{ NCCL_INSTALL_PATH }},{{ TENSORRT_INSTALL_PATH }}'

- debug:
    msg: "TensorFlow build logs saved to {{ TENSORFLOW_BUILD_DIR }}/build.log"

- name: Build pip package with Bazel
  shell: |
    cd {{ TENSORFLOW_BUILD_DIR }}
    ./bazel-bin/tensorflow/tools/pip_package/build_pip_package {{ TF_PIP_PACKAGE_LOCATION }}

- name: Shutdown bazel
  shell: bazel shutdown

- name: Remove bazel cache because it is very large and we don't need it anymore
  shell: rm -rf /root/.cache/bazel
