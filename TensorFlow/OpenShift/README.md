# OpenShift Files

## Overview

This folder contains files used for launching the [official TensorFlow High-Performance CNN benchmarks](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks) as an app in OpenShift on AWS. If you wish to build and run TensorFlow on RHEL 8, follow the instructions in the next section carefully. They are required. Otherwise, for RHEL 7 builds, you can skip to the **Basics** section.

## PREP WORK

### 1. Configure

The entire image build, s2i app build, benchmarks, etc. process is streamlined with a Makefile that is generated by the `configure` script in this directory. Before you do anything, run:

```
$ ./configure <flags>
```

For help on using `configure`, run:

```
$ ./configure --help
```

If you plan to use any registry.redhat.io images, follow step #2 below. Otherwise, skip to step #3.

### 2. Using registry.redhat.io Images (REQUIRED FOR CUDA RHEL 7 BUILDS + NON-CUDA RHEL 8 BUILDS)

Follow the instructions in the `setup` folder in this directory. In essence, you will need to run a create two files under **../../secrets**, then run a `make` command.

### 3. Installing Necessary Cluster Operators

After you've run `configure`, you should setup and install three necessary operators to your cluster: Special Resource Operator (SRO), Node Feature Discovery (NFD) Operator, and Ripsaw. To do so, run

```
$ make setup_operators
```

The SRO is used for installing NVIDIA drivers to your cluster, the NFD operator is used for discovering node features, and Ripsaw is for running the benchmarks.

### 4. Building and Using a CUDA "Base Image" from this Repository

To prepare for a CUDA build using a "base image," make sure to build one of the Docker images under `../Dockerfiles/custom/rhel7/cuda` or `../Dockerfiles/custom/rhel8/cuda`. Follow the instructions in the **setup** folder in this directory to do so. Such base images have cuDNN and NCCL pre-installed, which means you can skip the next subsection "Setting up an EBS Volume."

### 5. Setting up an EBS Volume (for Non Custom Base Images)

To prepare for CUDA builds in images which do *not* have cuDNN and NCCL preinstalled, you will first need to create an EBS volume, like so:

```
$ cd setup/volumes
$ sh create_ebs_volume.sh -n <volume_name> -t <volume_type> -s <volume_size> -z <aws_availability_zone>
```

Once you've created your volume, create a dummy pod that will be used for storing data in the EBS storage via a PV (Persistent Volume):

```
$ #cd setup/volumes
$ sh create_temp_nvidia_pod.sh <volume_id>
```

This will create a temporary pod named `tmp-nvidia-pod`, which you can access by executing:

```
$ oc exec -it tmp-nvidia-pod -- /bin/bash
```

The EBS volume will be mounted under `/tmp/nvidia_ebs`. From there, you can download your two (required) NVIDIA packages: (1.) NCCL, and (2.) cuDNN.

If you have an s3 bucket where the packages are stored, then install `awscli` via `pip` or `pip3`, configure it to provide your credentials, and download. Otherwise, download from wherever you have your tarballs hosted.

Once you're done, type `exit` to exit the pod. Because you won't be needing it anymore, you can delete it via:

```
$ oc delete pod/tmp-nvidia-pod
```

Now you're all set! The EBS volume should have your cuDNN and NCCL tar files!

#### Warning About Using ubi8 and Related Images

The RHEL 8 "ubi8" image has a limited set of packages that can be installed through its included repos. Even NVIDIA's `nvidia/cuda:<tag>` images reference ubi8, so the same issue lies there. Thus, in order to obtain the packages that *cannot* be installed through those repos, you will need to create your own image using one of the custom provided Dockerfiles in `../Dockerfiles/custom` and supply your own `.repo` files to point to RHEL 8 repositories.

## Preparing ImageNet

If you would like to use real ImageNet data, please follow the instructions in **setup/README.md** for ImageNet. This process requires using an EBS volume to 'host' your ImageNet data. It is very similar to the approach of creating a temporary NVIDIA pod for holding NVIDIA packages.

To create an ImageNet EBS volume,

```
$ cd setup/volumes
$ sh create_ebs_volume.sh -n <volume_name> -t <volume_type> -s <volume_size> -z <aws_availability_zone>
```

Once you've created your volume, create a dummy pod that will be used for storing data in the EBS storage via a PV (Persistent Volume):

```
$ #cd setup/volumes
$ sh create_temp_imagenet_pod.sh <volume_id>
```

From here, enter the pod via 

```
oc exec -it tmp-imagenet-pod -- /bin/bash
```

...then download/upload your ImageNet data to the mounted volume located at `/tmp/imagenet_ebs`. Delete the pod when you're done, if desired.

## How to Run the Benchmarks

By default, your OpenShift "base" image will be named `tensorflow-rhel7` for RHEL 7 or `tensorflow-rhel8` for RHEL 8, and will be saved to your exposed OpenShift image registry. (NOTE: You don't need to tell the `run_me.sh` script the link to your registry since the script automatically determines the link for you. However, if you have *multiple* registries for whatever reason, you may want to edit which registry to use. So, edit the `REGISTRY` variable.) Since s2i is used, there will be an additional image created. After the second image has been created, you can run the benchmarks.

Assuming you have already run `configure` to generate a Makefile, you can do everything at once by running

```
$ make
``

To build just the base imagestream,

```
$ make imagestream
```

To build just the s2i imagestream,

```
$ make s2i
```

To run just the benchmarks,

```
$ make benchmarks
```

## Automatically creating a Node

If you wish to create a MachineSet and run the pod on a node with a specific instance type, use `../../helper_scripts/OpenShift/create_machineset.sh` to create a YAML file. Or you can create your own YAML file. The script is provided as a convenience.

Once your YAML file has been generated,

```
$ oc create -f <YAML_filename>
```

If you would like information on how to use the script,

```
$ cd ../../helper_scripts/OpenShift
$ sh create_machineset.sh -h
```

To get your AMI ID and cluster ID, either log into your [AWS console](https://aws.amazon.com/console/) and find your cluster, **or** 

```
$ aws iam list-instance-profiles --output json | grep <your_cluster_name_or_partial_cluster_name> -B 18
            "InstanceProfileId": "<instance_profile_id>", 
            "Roles": [
                {
                    "AssumeRolePolicyDocument": {
                        "Version": "2012-10-17", 
                        "Statement": [
                            {
                                "Action": "sts:AssumeRole", 
                                "Principal": {
                                    "Service": "ec2.amazonaws.com"
                                }, 
                                "Effect": "Allow", 
                                "Sid": ""
                            }
                        ]
                    }, 
                    "RoleId": "<role_id>", 
                    "CreateDate": "2019-07-18T15:57:33Z", 
                    "RoleName": "<cluster_id>-worker-role", 
                    "Path": "/", 
                    "Arn": "arn:aws:iam:<id>:role/<cluster_id>-worker-role"
                }
            ], 
            "CreateDate": "2019-07-18T15:57:33Z", 
            "InstanceProfileName": "<cluster_id>-worker-profile", 
            "Path": "/", 
            "Arn": "arn:aws:iam::<id>:instance-profile/<cluster_id>-worker-profile"

$ aws ec2 describe-instances --filters "Name=iam-instance-profile.id,Values=<instance_profile_id>" --output json | grep ImageId
                    "ImageId": "ami-<hash>", 
                    "ImageId": "ami-<hash>", 
                    "ImageId": "ami-<hash>", 
```

## Advanced CPU Options (CPU Manager)

### Installing and Enabling CPU Manager

First, install and enable CPU Manager to your cluster. To do so,

```
$ sh ../../helper_scripts/OpenShift/enable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -x <avx_instruction_set>
```

or

```
$ sh ../../helper_scripts/OpenShift/enable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -i <instance_type>
```

### Uninstalling and Disabling CPU Manager

To uninstall,

```
$ sh ../../helper_scripts/OpenShift/disable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -x <avx_instruction_set>
```

or

```
$ sh ../../helper_scripts/OpenShift/disable_cpumanager.sh -n <node_name> -k /path/to/cpumanager-kubeletconfig.yaml -i <instance_type>
```
